{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'HE is walking with 3 dogs to SCHOOL.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare functions we need\n",
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Remove Stopwords & Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords from nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_nltk_en = set(stopwords.words('english'))\n",
    "\n",
    "# Stopwords from stopwords-json\n",
    "stopwords_json = {\"en\":[\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]}\n",
    "stopwords_json_en = set(stopwords_json['en'])\n",
    "\n",
    "# Punctuation\n",
    "from string import punctuation\n",
    "stopwords_punct = set(punctuation)\n",
    "\n",
    "# Combine the stopwords. Its a lot longer so I'm not printing it out...\n",
    "stoplist_combined = set.union(stopwords_json_en, stopwords_nltk_en, stopwords_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HE', 'walking', '3', 'dogs', 'SCHOOL']\n"
     ]
    }
   ],
   "source": [
    "# Testing: stopwords\n",
    "print([word for word in word_tokenize(sent) if word not in stoplist_combined])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_stopwords(sent):\n",
    "#     '''remove stopwords & digits'''\n",
    "#     return [word for word in word_tokenize(sent) if word not in stoplist_combined and not word.isdigit()]\n",
    "    \n",
    "# #     words = []\n",
    "# #     for word in word_tokenize(sent):\n",
    "# #         if word not in stoplist_combined and not word.isdigit():\n",
    "# #             words.append(word)\n",
    "# #     return words\n",
    "\n",
    "# remove_stopwords(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Lemmatization\n",
    "having -> have\n",
    "\n",
    "Lemmatization requires POS (Part of Speech) tag (e.g. noun, verb, adj., adv.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def penn2morphy(penntag):\n",
    "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
    "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "                  'VB':'v', 'RB':'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n' \n",
    "    \n",
    "def lemmatize_sent(sent): \n",
    "    # Text input is string, returns lowercased strings.\n",
    "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) \n",
    "            for word, tag in pos_tag(word_tokenize(sent))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'be', 'walk', 'with', '3', 'dog', 'to', 'school', '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing: lemmatize_sent()\n",
    "lemmatize_sent(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HE NNP\n",
      "is VBZ\n",
      "walking VBG\n",
      "with IN\n",
      "3 CD\n",
      "dogs NNS\n",
      "to TO\n",
      "SCHOOL NNP\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "for word, tag in pos_tag(word_tokenize(sent)):\n",
    "    print (word, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) preprocess_text = Lemmatize + Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    '''lemmatize_sent + remove stopwords'''\n",
    "    # Input: str, i.e. document/sentence\n",
    "    # Output: list(str) , i.e. list of lemmas\n",
    "    return [word for word in lemmatize_sent(text) \n",
    "            if word not in stoplist_combined\n",
    "            and not word.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['walk', 'dog', 'school']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start eating pizza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./input/train.json') as fin:\n",
    "    trainjson = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'giver_username_if_known': 'N/A',\n",
       " 'number_of_downvotes_of_request_at_retrieval': 0,\n",
       " 'number_of_upvotes_of_request_at_retrieval': 1,\n",
       " 'post_was_edited': False,\n",
       " 'request_id': 't3_l25d7',\n",
       " 'request_number_of_comments_at_retrieval': 0,\n",
       " 'request_text': 'Hi I am in need of food for my 4 children we are a military family that has really hit hard times and we have exahusted all means of help just to be able to feed my family and make it through another night is all i ask i know our blessing is coming so whatever u can find in your heart to give is greatly appreciated',\n",
       " 'request_text_edit_aware': 'Hi I am in need of food for my 4 children we are a military family that has really hit hard times and we have exahusted all means of help just to be able to feed my family and make it through another night is all i ask i know our blessing is coming so whatever u can find in your heart to give is greatly appreciated',\n",
       " 'request_title': 'Request Colorado Springs Help Us Please',\n",
       " 'requester_account_age_in_days_at_request': 0.0,\n",
       " 'requester_account_age_in_days_at_retrieval': 792.4204050925925,\n",
       " 'requester_days_since_first_post_on_raop_at_request': 0.0,\n",
       " 'requester_days_since_first_post_on_raop_at_retrieval': 792.4204050925925,\n",
       " 'requester_number_of_comments_at_request': 0,\n",
       " 'requester_number_of_comments_at_retrieval': 0,\n",
       " 'requester_number_of_comments_in_raop_at_request': 0,\n",
       " 'requester_number_of_comments_in_raop_at_retrieval': 0,\n",
       " 'requester_number_of_posts_at_request': 0,\n",
       " 'requester_number_of_posts_at_retrieval': 1,\n",
       " 'requester_number_of_posts_on_raop_at_request': 0,\n",
       " 'requester_number_of_posts_on_raop_at_retrieval': 1,\n",
       " 'requester_number_of_subreddits_at_request': 0,\n",
       " 'requester_received_pizza': False,\n",
       " 'requester_subreddits_at_request': [],\n",
       " 'requester_upvotes_minus_downvotes_at_request': 0,\n",
       " 'requester_upvotes_minus_downvotes_at_retrieval': 1,\n",
       " 'requester_upvotes_plus_downvotes_at_request': 0,\n",
       " 'requester_upvotes_plus_downvotes_at_retrieval': 1,\n",
       " 'requester_user_flair': None,\n",
       " 'requester_username': 'nickylvst',\n",
       " 'unix_timestamp_of_request': 1317852607.0,\n",
       " 'unix_timestamp_of_request_utc': 1317849007.0}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainjson[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UID:\t t3_l25d7 \n",
      "\n",
      "Title:\t Request Colorado Springs Help Us Please \n",
      "\n",
      "Text:\t Hi I am in need of food for my 4 children we are a military family that has really hit hard times and we have exahusted all means of help just to be able to feed my family and make it through another night is all i ask i know our blessing is coming so whatever u can find in your heart to give is greatly appreciated \n",
      "\n",
      "Tag:\t False\n"
     ]
    }
   ],
   "source": [
    "print('UID:\\t', trainjson[0]['request_id'], '\\n')\n",
    "print('Title:\\t', trainjson[0]['request_title'], '\\n')\n",
    "print('Text:\\t', trainjson[0]['request_text_edit_aware'], '\\n')\n",
    "print('Tag:\\t', trainjson[0]['requester_received_pizza'], end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert json to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_id</th>\n",
       "      <th>request_title</th>\n",
       "      <th>request_text_edit_aware</th>\n",
       "      <th>requester_received_pizza</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_l25d7</td>\n",
       "      <td>Request Colorado Springs Help Us Please</td>\n",
       "      <td>Hi I am in need of food for my 4 children we a...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_rcb83</td>\n",
       "      <td>[Request] California, No cash and I could use ...</td>\n",
       "      <td>I spent the last money I had on gas today. Im ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_lpu5j</td>\n",
       "      <td>[Request] Hungry couple in Dundee, Scotland wo...</td>\n",
       "      <td>My girlfriend decided it would be a good idea ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_mxvj3</td>\n",
       "      <td>[Request] In Canada (Ontario), just got home f...</td>\n",
       "      <td>It's cold, I'n hungry, and to be completely ho...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_1i6486</td>\n",
       "      <td>[Request] Old friend coming to visit. Would LO...</td>\n",
       "      <td>hey guys:\\n I love this sub. I think it's grea...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  request_id                                      request_title  \\\n",
       "0   t3_l25d7            Request Colorado Springs Help Us Please   \n",
       "1   t3_rcb83  [Request] California, No cash and I could use ...   \n",
       "2   t3_lpu5j  [Request] Hungry couple in Dundee, Scotland wo...   \n",
       "3   t3_mxvj3  [Request] In Canada (Ontario), just got home f...   \n",
       "4  t3_1i6486  [Request] Old friend coming to visit. Would LO...   \n",
       "\n",
       "                             request_text_edit_aware  requester_received_pizza  \n",
       "0  Hi I am in need of food for my 4 children we a...                     False  \n",
       "1  I spent the last money I had on gas today. Im ...                     False  \n",
       "2  My girlfriend decided it would be a good idea ...                     False  \n",
       "3  It's cold, I'n hungry, and to be completely ho...                     False  \n",
       "4  hey guys:\\n I love this sub. I think it's grea...                     False  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.io.json.json_normalize(trainjson) # Pandas magic... \n",
    "df_train = df[['request_id', 'request_title', \n",
    "               'request_text_edit_aware', \n",
    "               'requester_received_pizza']]\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./input/test.json') as fin:\n",
    "    testjson = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UID:\t t3_i8iy4 \n",
      "\n",
      "Title:\t [request] pregger gf 95 degree house and no food.. promise to pay it forward! Northern Colorado \n",
      "\n",
      "Text:\t Hey all! It's about 95 degrees here and our kitchen is pretty much empty save for some bread and cereal.  My girlfriend/fiance is 8 1/2 months pregnant and we could use a good meal.  We promise to pay it forward when we get money! Thanks so much in advance! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('UID:\\t', testjson[0]['request_id'], '\\n')\n",
    "print('Title:\\t', testjson[0]['request_title'], '\\n')\n",
    "print('Text:\\t', testjson[0]['request_text_edit_aware'], '\\n')\n",
    "#print('Tag:\\t', testjson[0]['requester_received_pizza'], end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert json to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_id</th>\n",
       "      <th>request_title</th>\n",
       "      <th>request_text_edit_aware</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_i8iy4</td>\n",
       "      <td>[request] pregger gf 95 degree house and no fo...</td>\n",
       "      <td>Hey all! It's about 95 degrees here and our ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_1mfqi0</td>\n",
       "      <td>[Request] Lost my job day after labour day, st...</td>\n",
       "      <td>I didn't know a place like this exists! \\n\\nI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_lclka</td>\n",
       "      <td>(Request) pizza for my kids please?</td>\n",
       "      <td>Hi Reddit. Im a single dad having a really rou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_1jdgdj</td>\n",
       "      <td>[Request] Just moved to a new state(Waltham MA...</td>\n",
       "      <td>Hi I just moved to Waltham MA from my home sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_t2qt4</td>\n",
       "      <td>[Request] Two girls in between paychecks, we'v...</td>\n",
       "      <td>We're just sitting here near indianapolis on o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  request_id                                      request_title  \\\n",
       "0   t3_i8iy4  [request] pregger gf 95 degree house and no fo...   \n",
       "1  t3_1mfqi0  [Request] Lost my job day after labour day, st...   \n",
       "2   t3_lclka                (Request) pizza for my kids please?   \n",
       "3  t3_1jdgdj  [Request] Just moved to a new state(Waltham MA...   \n",
       "4   t3_t2qt4  [Request] Two girls in between paychecks, we'v...   \n",
       "\n",
       "                             request_text_edit_aware  \n",
       "0  Hey all! It's about 95 degrees here and our ki...  \n",
       "1  I didn't know a place like this exists! \\n\\nI ...  \n",
       "2  Hi Reddit. Im a single dad having a really rou...  \n",
       "3  Hi I just moved to Waltham MA from my home sta...  \n",
       "4  We're just sitting here near indianapolis on o...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.io.json.json_normalize(testjson) # Pandas magic... \n",
    "df_test = df[['request_id', 'request_title', \n",
    "               'request_text_edit_aware']]\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have df_train & df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape: (3232, 4)\n",
      "val.shape: (808, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "train, val = train_test_split(df_train, test_size=0.2)\n",
    "print('train.shape:', train.shape)\n",
    "print('val.shape:', val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize the train, val and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    fit_transform: train\n",
    "    tranform:      val & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the vectorizer and \n",
    "# override the analyzer totally with the preprocess_text().\n",
    "# Note: the vectorizer is just an 'empty' object now.\n",
    "count_vect = CountVectorizer(analyzer=preprocess_text)\n",
    "\n",
    "# When we use `CounterVectorizer.fit_transform`,\n",
    "# we essentially create the dictionary and \n",
    "# vectorize our input text at the same time.\n",
    "train_set = count_vect.fit_transform(train['request_text_edit_aware'])   # sparse matrix\n",
    "train_tags = train['requester_received_pizza']\n",
    "\n",
    "# When vectorizing the validation data, we use `CountVectorizer.transform()`.\n",
    "val_set = count_vect.transform(val['request_text_edit_aware'])\n",
    "val_tags = val['requester_received_pizza']\n",
    "\n",
    "# When vectorizing the test data, we use `CountVectorizer.transform()`.\n",
    "test_set = count_vect.transform(df_test['request_text_edit_aware'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set.shape: (3232, 9570)\n",
      "train_tags.shape: (3232,)\n",
      "\n",
      "val_set.shape: (808, 9570)\n",
      "val_tags.shape: (808,)\n",
      "\n",
      "test_set.shape: (1631, 11075)\n"
     ]
    }
   ],
   "source": [
    "print('train_set.shape:', train_set.shape)\n",
    "print('train_tags.shape:', train_tags.shape)\n",
    "print()\n",
    "print('val_set.shape:', val_set.shape)\n",
    "print('val_tags.shape:', val_tags.shape)\n",
    "print()\n",
    "print('test_set.shape:', test_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Naive Bayes classifier in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB() \n",
    "\n",
    "# To train the classifier, simple do \n",
    "clf.fit(train_set, train_tags) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prediction\n",
    "\n",
    "# To predict our tags (i.e. whether requesters get their pizza), \n",
    "# we feed the vectorized `test_set` to .predict()\n",
    "predictions_valid = clf.predict(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pizza reception accuracy = 73.76237623762376\n"
     ]
    }
   ],
   "source": [
    "## Evaluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Pizza reception accuracy = {}'.format(\n",
    "        accuracy_score(predictions_valid, valid_tags) * 100)\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets use the full training data set and re-vectorize and retrain the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vectorizing\n",
    "\n",
    "count_vect = CountVectorizer(analyzer=preprocess_text)\n",
    "\n",
    "full_train_set = count_vect.fit_transform(df_train['request_text_edit_aware'])   # train+val\n",
    "full_tags = df_train['requester_received_pizza']\n",
    "\n",
    "# Note: We have to re-vectorize the test set since\n",
    "#       now our vectorizer is different using the full \n",
    "#       training set.\n",
    "test_set = count_vect.transform(df_test['request_text_edit_aware'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training\n",
    "\n",
    "# Reset the classifier\n",
    "clf = MultinomialNB() \n",
    "clf.fit(full_train_set, full_tags) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prediction\n",
    "\n",
    "predictions = clf.predict(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Since we don't have the `requester_received_pizza` field in test data, we can't measure accuracy. But we can do some exploration as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From the training data, we had 24% pizza giving rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of 4040 requests, only 994 gets their pizzas, 24.603960396039604% success rate...\n"
     ]
    }
   ],
   "source": [
    "success_rate = sum(df_train['requester_received_pizza']) / len(df_train) * 100\n",
    "print(str('Of {} requests, only {} gets their pizzas,'\n",
    "          ' {}% success rate...'.format(len(df_train), \n",
    "                                        sum(df_train['requester_received_pizza']), \n",
    "                                       success_rate)\n",
    "         )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## our classifier give 3% success rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of 1631 requests, only 51 gets their pizzas, 3.126916002452483% success rate...\n"
     ]
    }
   ],
   "source": [
    "success_rate = sum(predictions) / len(predictions) * 100\n",
    "print(str('Of {} requests, only {} gets their pizzas,'\n",
    "          ' {}% success rate...'.format(len(predictions), \n",
    "                                        sum(predictions), \n",
    "                                       success_rate)\n",
    "         )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_id</th>\n",
       "      <th>requester_received_pizza</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_i8iy4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_1mfqi0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_lclka</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_1jdgdj</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_t2qt4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  request_id  requester_received_pizza\n",
       "0   t3_i8iy4                         0\n",
       "1  t3_1mfqi0                         0\n",
       "2   t3_lclka                         0\n",
       "3  t3_1jdgdj                         0\n",
       "4   t3_t2qt4                         0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample_submission = pd.read_csv('./input/sampleSubmission.csv')\n",
    "df_sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_id</th>\n",
       "      <th>requester_received_pizza</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_i8iy4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_1mfqi0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_lclka</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_1jdgdj</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_t2qt4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  request_id  requester_received_pizza\n",
       "0   t3_i8iy4                         0\n",
       "1  t3_1mfqi0                         0\n",
       "2   t3_lclka                         0\n",
       "3  t3_1jdgdj                         0\n",
       "4   t3_t2qt4                         0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We've kept the `request_id` previous in the `df_test` dataframe.\n",
    "# We can simply merge that column with our predictions.\n",
    "df_output = pd.DataFrame({'request_id': list(df_test['request_id']), \n",
    "                          'requester_received_pizza': list(predictions)}\n",
    "                        )\n",
    "# Convert the predictions from boolean to integer.\n",
    "df_output['requester_received_pizza'] = df_output['requester_received_pizza'].astype(int)\n",
    "df_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the csv file.\n",
    "df_output.to_csv('./output/basic-nlp-submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
